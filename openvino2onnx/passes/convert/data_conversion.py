"""
Copyright Wenyi Tang 2024-2025

:Author: Wenyi Tang
:Email: wenyitang@outlook.com

"""

# pylint: disable=arguments-differ

import itertools
import re

import onnx
from onnx import TensorProto
from onnx.helper import make_attribute
from onnx.numpy_helper import from_array, to_array

from openvino2onnx.graph import OnnxGraph
from openvino2onnx.passes import L1, PASSES, logger
from openvino2onnx.passes.utils import cast_in


@L1.register()
def half_to_float(graph: OnnxGraph) -> OnnxGraph:
    """Convert half consts and values to float32."""

    for node in graph:
        node_pb = graph.nodes[node]["pb"]
        if node_pb.op_type == "Constant":
            tensor = node_pb.attribute[0].t
            if tensor.data_type == TensorProto.FLOAT16:
                array = to_array(tensor).astype("float32")
                attr = make_attribute(key="value", value=from_array(array))
                node_pb.attribute.pop()
                node_pb.attribute.append(attr)
        elif node_pb.op_type == "Cast":
            # if cast target is fp16, change it to fp32
            for attr in node_pb.attribute:
                if attr.name == "to" and attr.i == TensorProto.FLOAT16:
                    attr.i = TensorProto.FLOAT
    for init in graph.initializer:
        if init.data_type == TensorProto.FLOAT16:
            array = to_array(init).astype("float32")
            init.data_type = TensorProto.FLOAT
            init.raw_data = from_array(array).raw_data
    for io in itertools.chain(graph.input, graph.output):
        if io.type.tensor_type.elem_type == TensorProto.FLOAT16:
            io.type.tensor_type.elem_type = TensorProto.FLOAT
    return graph


def canonicalize_resize(graph: OnnxGraph, op_name: str):
    """The scales input of Resize must be float32."""
    assert op_name in graph.nodes
    node_pb = graph.nodes[op_name]["pb"]
    scales = node_pb.input[2]
    if scales:
        graph.add_onnx_node(cast_in(node_pb, 2, TensorProto.FLOAT))


def dtype_canonicalize(graph: OnnxGraph, op_type: str, op_name: str):
    """Canonicalize data type for operators that don't support fp16."""
    if op_type == "Resize":
        canonicalize_resize(graph, op_name)
    # TODO: add more operators here


@PASSES.register()
def float_to_half(graph: OnnxGraph) -> OnnxGraph:
    """Convert float32 consts and values to half."""

    for node in graph:
        node_pb = graph.nodes[node]["pb"]
        if node_pb.op_type == "Constant":
            tensor = node_pb.attribute[0].t
            if tensor.data_type == TensorProto.FLOAT:
                array = to_array(tensor).astype("float16")
                attr = make_attribute(key="value", value=from_array(array))
                node_pb.attribute.pop()
                node_pb.attribute.append(attr)
    for init in graph.initializer:
        if init.data_type == TensorProto.FLOAT:
            array = to_array(init).astype("float16")
            init.data_type = TensorProto.FLOAT16
            init.raw_data = from_array(array).raw_data
    for io in itertools.chain(graph.input, graph.output):
        if io.type.tensor_type.elem_type == TensorProto.FLOAT:
            io.type.tensor_type.elem_type = TensorProto.FLOAT16
    # fix operators that don't support fp16
    try:
        onnx.checker.check_model(graph.model, full_check=True)
    except onnx.shape_inference.InferenceError as e:
        op_type = re.findall(r"op_type\:(\w+)", f"{e}")[0]
        op_name = re.findall(r"node name\: (.*)\)", f"{e}")[0]
        logger.debug(f"Operator {op_name}({op_type}) check failed, fixing it...")
        dtype_canonicalize(graph, op_type, op_name)
    return graph
